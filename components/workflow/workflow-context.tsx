// workflow-context.tsx
"use client";
import type React from "react";
import {
  createContext,
  useContext,
  useState,
  useCallback,
  useEffect,
} from "react";
import { v4 as uuidv4 } from "uuid";
import type { NodeType, SchemaItem } from "@/services/interface"; // Ensure NodeType includes all your types
import { useToast } from "@/components/ui/use-toast";
import {
  createFileConversionConfig,
  updateDag,
  triggerDagRun,
  makePythonSafeId,
} from "@/services/file-conversion-service";
import {
  createCliOperatorConfig,
  mapCopyFileToCliOperator,
  mapMoveFileToCliOperator,
  mapRenameFileToCliOperator,
  mapDeleteFileToCliOperator,
} from "@/services/cli-operator-service"; // Added CLI service imports

const baseurl = process.env.NEXT_PUBLIC_USER_API_END_POINT;

export type NodeStatus =
  | "idle"
  | "running"
  | "success"
  | "error"
  | "configured";

export interface NodePosition {
  x: number;
  y: number;
}

export interface NodeSchema {
  label: string;
  description: string;
  inputSchema: SchemaItem[];
  outputSchema: SchemaItem[];
}

export interface WorkflowNodeData {
  label?: string;
  displayName?: string;
  filename?: string;
  content?: string;
  textContent?: string;
  toFilename?: string;
  sourceFilename?: string;
  targetFilename?: string;
  overwrite?: boolean;
  isDirectory?: boolean;
  includeTimestamp?: boolean;
  encoding?: string;
  readAs?: string;
  excludeContent?: boolean;
  append?: boolean;
  writeAs?: string;
  addLineSeparator?: boolean;
  includeSubDirectories?: boolean;
  createNonExistingDirs?: boolean;
  mode?: string;
  language?: string;
  code?: string;
  recursive?: boolean;
  directory?: string;
  filter?: any;
  interval?: number;
  path?: string;
  method?: string;
  port?: number;
  url?: string;
  headers?: Record<string, string>;
  body?: any;
  timeout?: number;
  options?: Record<string, any>;
  jsonObject?: object;
  xmlString?: string;
  inputSchema?: string;
  outputSchema?: string;
  oldFilename?: string;
  newFilename?: string;
  active?: boolean;
  provider?: string;
  format?: string;
  schema?: any;
  order_by?: any;
  aggregation?: any;
  // CLI operator specific fields
  source_path?: string;
  destination_path?: string;
  // Database specific fields
  connectionString?: string;
  writeMode?: string;
  table?: string;
  user?: string;
  password?: string;
  batchSize?: string;
  query?: string; // Crucial for database source node
  // Source specific fields (might overlap)
  filePath?: string; // Potentially for a generic 'source' that isn't DB
  csvOptions?: Record<string, any>;
  // No need for cliOperatorPayload here, it's generated by mapping functions
}

export interface WorkflowNode {
  id: string;
  type: NodeType;
  position: NodePosition;
  data: WorkflowNodeData;
  status?: NodeStatus;
  output?: any;
  error?: string;
}

export interface NodeConnection {
  id: string;
  sourceId: string;
  targetId: string;
  sourceHandle?: string;
  targetHandle?: string;
}

export interface LogEntry {
  id: string;
  nodeId: string;
  nodeName: string;
  timestamp: Date;
  status: NodeStatus | "info";
  message: string;
  details?: any;
}

export interface DAG {
  id: number;
  name: string;
  dag_id: string;
  schedule: string | null;
  active: boolean;
  dag_sequence: Array<{
    id: string;
    type: string;
    config_id: number;
    next: string[];
  }>;
  active_dag_run: number | null;
  created_at: string;
  updated_at: string;
}

interface WorkflowContextType {
  nodes: WorkflowNode[];
  connections: NodeConnection[];
  logs: LogEntry[];
  selectedNodeId: string | null;
  pendingConnection: { sourceId: string; sourceHandle?: string } | null;
  propertiesModalNodeId: string | null;
  dataMappingModalNodeId: string | null;
  draggingNodeInfo: { id: string; offset: { x: number; y: number } } | null;
  setPendingConnection: (
    connection: { sourceId: string; sourceHandle?: string } | null
  ) => void;
  setPropertiesModalNodeId: (nodeId: string | null) => void;
  setDataMappingModalNodeId: (nodeId: string | null) => void;
  setDraggingNodeInfo: (
    info: { id: string; offset: { x: number; y: number } } | null
  ) => void;
  addNode: (
    type: NodeType,
    position: NodePosition,
    initialData?: Partial<WorkflowNodeData>
  ) => string;
  updateNode: (
    id: string,
    updates: Partial<Omit<WorkflowNode, "data">> & {
      data?: Partial<WorkflowNodeData>;
    }
  ) => void;
  removeNode: (id: string) => void;
  selectNode: (id: string | null) => void;
  addConnection: (
    sourceId: string,
    targetId: string,
    sourceHandle?: string,
    targetHandle?: string
  ) => void;
  removeConnection: (connectionId: string) => void;
  clearWorkflow: () => void;
  saveWorkflow: () => { nodes: WorkflowNode[]; connections: NodeConnection[] };
  saveWorkflowToBackend: () => Promise<void>;
  loadWorkflow: (data: {
    nodes: WorkflowNode[];
    connections: NodeConnection[];
  }) => void;
  runWorkflow: () => Promise<void>;
  executeNode: (nodeId: string, inputData?: any) => Promise<any>;
  addLog: (log: Omit<LogEntry, "id" | "timestamp">) => void;
  clearLogs: () => void;
  getNodeById: (id: string) => WorkflowNode | undefined;
  getCurrentWorkflowId: () => string | null;
  saveAndRunWorkflow: () => Promise<void>;
}

const WorkflowContext = createContext<WorkflowContextType | undefined>(
  undefined
);

export const getCurrentClientId = (): string | null => {
  try {
    const clientDataString = localStorage.getItem("currentClient");
    if (clientDataString) {
      const parsedClient = JSON.parse(clientDataString);
      if (parsedClient?.id && String(parsedClient.id).trim() !== "") {
        return String(parsedClient.id);
      }
    }
    const workflowDataString = localStorage.getItem("currentWorkflow");
    if (workflowDataString) {
      const parsedWorkflow = JSON.parse(workflowDataString);
      if (
        parsedWorkflow?.client_id &&
        String(parsedWorkflow.client_id).trim() !== ""
      ) {
        return String(parsedWorkflow.client_id);
      }
    }
    console.warn("getCurrentClientId: No valid client_id found.");
  } catch (error) {
    console.error("getCurrentClientId: Error accessing localStorage:", error);
  }
  return null;
};

export function WorkflowProvider({ children }: { children: React.ReactNode }) {
  const [nodes, setNodes] = useState<WorkflowNode[]>([]);
  const [connections, setConnections] = useState<NodeConnection[]>([]);
  const [logs, setLogs] = useState<LogEntry[]>([]);
  const [selectedNodeId, setSelectedNodeId] = useState<string | null>(null);
  const [isRunning, setIsRunning] = useState(false);
  const [isSaving, setIsSaving] = useState(false);
  const [pendingConnection, setPendingConnection] = useState<{
    sourceId: string;
    sourceHandle?: string;
  } | null>(null);
  const [propertiesModalNodeId, setPropertiesModalNodeId] = useState<
    string | null
  >(null);
  const [dataMappingModalNodeId, setDataMappingModalNodeId] = useState<
    string | null
  >(null);
  const [draggingNodeInfo, setDraggingNodeInfo] = useState<{
    id: string;
    offset: { x: number; y: number };
  } | null>(null);
  const { toast } = useToast();

  const addNode = useCallback(
    (
      type: NodeType,
      position: NodePosition,
      initialData?: Partial<WorkflowNodeData>
    ) => {
      const displayName =
        initialData?.displayName ||
        `${type}_${Math.floor(Math.random() * 10000)}`;
      const nodeId = makePythonSafeId(displayName);
      const newNode: WorkflowNode = {
        id: nodeId,
        type,
        position,
        data: { label: type, displayName, active: true, ...initialData },
        status: "idle",
      };
      setNodes((prev) => [...prev, newNode]);
      return newNode.id;
    },
    []
  );

  const updateNode = useCallback(
    (
      id: string,
      updates: Partial<Omit<WorkflowNode, "data">> & {
        data?: Partial<WorkflowNodeData>;
      }
    ) => {
      setNodes((prevNodes) =>
        prevNodes.map((node) =>
          node.id === id
            ? { ...node, ...updates, data: { ...node.data, ...updates.data } }
            : node
        )
      );
    },
    []
  );

  const removeNode = useCallback(
    (id: string) => {
      setNodes((prev) => prev.filter((node) => node.id !== id));
      setConnections((prev) =>
        prev.filter((conn) => conn.sourceId !== id && conn.targetId !== id)
      );
      if (selectedNodeId === id) setSelectedNodeId(null);
      if (propertiesModalNodeId === id) setPropertiesModalNodeId(null);
    },
    [selectedNodeId, propertiesModalNodeId]
  );

  const selectNode = useCallback((id: string | null) => {
    setSelectedNodeId(id);
  }, []);

  const addConnection = useCallback(
    (
      sourceId: string,
      targetId: string,
      sourceHandle?: string,
      targetHandle?: string
    ) => {
      if (sourceId === targetId) return;
      const exists = connections.some(
        (conn) =>
          conn.sourceId === sourceId &&
          conn.targetId === targetId &&
          conn.sourceHandle === sourceHandle &&
          conn.targetHandle === targetHandle
      );
      if (exists) return;
      const isCircular = connections.some(
        (conn) => conn.sourceId === targetId && conn.targetId === sourceId
      );
      if (isCircular) {
        console.warn("Preventing direct circular connection");
        return;
      }
      const newConnection: NodeConnection = {
        id: uuidv4(),
        sourceId,
        targetId,
        sourceHandle,
        targetHandle,
      };
      setConnections((prev) => [...prev, newConnection]);
    },
    [connections]
  );

  const removeConnection = useCallback((connectionId: string) => {
    setConnections((prev) => prev.filter((conn) => conn.id !== connectionId));
  }, []);

  const clearWorkflow = useCallback(() => {
    setNodes([]);
    setConnections([]);
    setSelectedNodeId(null);
    setPropertiesModalNodeId(null);
    setPendingConnection(null);
    setDraggingNodeInfo(null);
    // clearLogs(); // Assuming clearLogs is defined
  }, []); // Removed clearLogs if not defined, or ensure it is

  const getCurrentWorkflowId = useCallback(() => {
    try {
      const workflowData = localStorage.getItem("currentWorkflow");
      if (workflowData) {
        const parsed = JSON.parse(workflowData);
        return parsed.dag_id;
      }
    } catch (error) {
      console.error("Error getting current workflow ID:", error);
    }
    return null;
  }, []);

  const convertWorkflowToDAG = useCallback(() => {
    const nodeConnectionMap: Record<string, string[]> = {};
    nodes.forEach((node) => {
      nodeConnectionMap[node.id] = [];
    });
    connections.forEach((connection) => {
      if (nodeConnectionMap[connection.sourceId]) {
        nodeConnectionMap[connection.sourceId].push(connection.targetId);
      }
    });
    const sanitizeId = (id: string) => {
      let safeId = id.replace(/[^a-zA-Z0-9_]/g, "_");
      if (!/^[a-zA-Z_]/.test(safeId)) safeId = "task_" + safeId;
      return safeId;
    };
    const dagSequence = nodes.map((node) => ({
      id: sanitizeId(node.id),
      type: node.type,
      config_id: 1,
      next: (nodeConnectionMap[node.id] || []).map(sanitizeId),
    }));
    return { dag_sequence: dagSequence };
  }, [nodes, connections]);

  const saveWorkflowToBackend = useCallback(async () => {
    let workflowId = getCurrentWorkflowId();
    if (!workflowId) workflowId = "dag_sample_47220ca3"; // Default if not found

    const localSaveWorkflow = () => {
      // Renamed to avoid conflict with context's saveWorkflow
      const workflowData = { nodes, connections };
      try {
        localStorage.setItem("workflowData", JSON.stringify(workflowData));
        console.log("Workflow data saved to localStorage.");
      } catch (error) {
        console.error("Failed to save workflow data to localStorage:", error);
      }
      return workflowData;
    };

    if (!workflowId) {
      toast({
        title: "Error",
        description: "No active workflow. Please create one first.",
        variant: "destructive",
      });
      return;
    }
    if (nodes.length === 0) {
      toast({
        title: "Error",
        description: "Cannot save an empty workflow.",
        variant: "destructive",
      });
      return;
    }
    setIsSaving(true);
    try {
      const dagData = convertWorkflowToDAG();
      const response = await fetch(`${baseurl}/dags/${workflowId}`, {
        method: "PUT",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(dagData),
      });
      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(
          errorData.detail || "Failed to save workflow to backend"
        );
      }
      localSaveWorkflow(); // Save to localStorage as well
      toast({
        title: "Success",
        description: "Workflow saved successfully to backend.",
        variant: "default",
      });
    } catch (error) {
      console.error("Error saving workflow to backend:", error);
      toast({
        title: "Error",
        description:
          error instanceof Error ? error.message : "Failed to save workflow.",
        variant: "destructive",
      });
    } finally {
      setIsSaving(false);
    }
  }, [nodes, connections, convertWorkflowToDAG, toast, getCurrentWorkflowId]);

  const saveWorkflow = useCallback(() => {
    // This is the context's saveWorkflow, typically for local save
    const workflowData = { nodes, connections };
    try {
      localStorage.setItem("workflowData", JSON.stringify(workflowData));
      console.log("Workflow saved to localStorage (context saveWorkflow).");
    } catch (error) {
      console.error("Failed to save workflow (context saveWorkflow):", error);
    }
    return workflowData;
  }, [nodes, connections]);

  const loadWorkflow = useCallback(
    (data: { nodes: WorkflowNode[]; connections: NodeConnection[] }) => {
      if (
        data?.nodes &&
        Array.isArray(data.nodes) &&
        data.connections &&
        Array.isArray(data.connections)
      ) {
        setNodes(data.nodes);
        setConnections(data.connections);
        setSelectedNodeId(null);
        setPropertiesModalNodeId(null);
        setPendingConnection(null);
        setDraggingNodeInfo(null);
        console.log("Workflow loaded from data.");
      } else {
        console.error("Invalid data format for loading workflow.");
      }
    },
    []
  );

  const addLog = useCallback((log: Omit<LogEntry, "id" | "timestamp">) => {
    const newLog: LogEntry = { ...log, id: uuidv4(), timestamp: new Date() };
    setLogs((prev) => [newLog, ...prev.slice(0, 99)]);
  }, []);

  const clearLogs = useCallback(() => {
    setLogs([]);
  }, []); // Defined clearLogs

  const getNodeById = useCallback(
    (id: string) => nodes.find((node) => node.id === id),
    [nodes]
  );

  const executeNode = useCallback(
    async (nodeId: string, inputData?: any): Promise<any> => {
      const node = getNodeById(nodeId);
      if (!node) {
        console.warn(`Node ${nodeId} not found.`);
        return null;
      }
      if (node.data?.active === false) {
        addLog({
          nodeId,
          nodeName: `${node.data?.label || node.type} (inactive)`,
          status: "info",
          message: "Skipping inactive node.",
        });
        const outgoing = connections.filter((c) => c.sourceId === nodeId);
        let lastOutput = inputData;
        for (const conn of outgoing) {
          lastOutput = await executeNode(conn.targetId, inputData);
        }
        return lastOutput;
      }
      updateNode(nodeId, { status: "running" });
      addLog({
        nodeId,
        nodeName: node.data?.label || node.type,
        status: "running",
        message: "Executing...",
        details: { input: inputData },
      });
      try {
        await new Promise((resolve) =>
          setTimeout(resolve, Math.random() * 100 + 50)
        ); // Faster simulation
        let output: any;
        const nodeData = node.data || {};
        // Simplified switch for brevity, expand as in your original
        switch (node.type) {
          case "start":
            output = { trigger: "manual", ...(inputData || {}) };
            break;
          case "read-file":
            output = { content: `Content of ${nodeData.path}` };
            break;
          case "write-file":
            output = { filePath: nodeData.path, written: true };
            break;
          case "source":
            output = {
              data: [{ id: 1, name: "Sample DB Data" }],
              source: nodeData.table || nodeData.query,
            };
            break;
          case "database":
            output = { success: true, table: nodeData.table };
            break;
          case "end":
            output = { finalStatus: "completed", result: inputData };
            break;
          default:
            output = { ...inputData, [`${node.type}_processed`]: true };
        }
        updateNode(nodeId, { status: "success", output, error: undefined });
        addLog({
          nodeId,
          nodeName: node.data?.label || node.type,
          status: "success",
          message: "Executed.",
          details: { output },
        });
        const outgoing = connections.filter((c) => c.sourceId === nodeId);
        let lastOutput = output;
        for (const conn of outgoing) {
          lastOutput = await executeNode(conn.targetId, output);
        }
        return lastOutput;
      } catch (error) {
        const msg = error instanceof Error ? error.message : String(error);
        updateNode(nodeId, { status: "error", error: msg, output: undefined });
        addLog({
          nodeId,
          nodeName: node.data?.label || node.type,
          status: "error",
          message: `Error: ${msg}`,
          details: { error },
        });
        throw error;
      }
    },
    [nodes, connections, getNodeById, updateNode, addLog]
  );

  const runWorkflow = useCallback(async () => {
    if (isRunning) {
      console.warn("Workflow already running.");
      return;
    }
    setIsRunning(true);
    addLog({
      nodeId: "system",
      nodeName: "System",
      status: "info",
      message: "Workflow started.",
    });
    setNodes((prev) =>
      prev.map((n) => ({
        ...n,
        status: "idle",
        output: undefined,
        error: undefined,
      }))
    );
    clearLogs();
    const activeStartNodes = nodes.filter(
      (n) => n.type === "start" && n.data?.active !== false
    );
    if (activeStartNodes.length === 0) {
      addLog({
        nodeId: "system",
        nodeName: "System",
        status: "error",
        message: "No active start nodes.",
      });
      setIsRunning(false);
      return;
    }
    try {
      await Promise.all(
        activeStartNodes.map((startNode) => executeNode(startNode.id))
      );
      addLog({
        nodeId: "system",
        nodeName: "System",
        status: "info",
        message: "Workflow finished.",
      });
    } catch (error) {
      const msg = error instanceof Error ? error.message : String(error);
      addLog({
        nodeId: "system",
        nodeName: "System",
        status: "error",
        message: `Workflow failed: ${msg}`,
      });
    } finally {
      setIsRunning(false);
    }
  }, [nodes, executeNode, isRunning, clearLogs, addLog]);

  useEffect(() => {
    try {
      const savedData = localStorage.getItem("workflowData");
      if (savedData) {
        const parsedData = JSON.parse(savedData);
        if (
          parsedData?.nodes &&
          Array.isArray(parsedData.nodes) &&
          parsedData.connections &&
          Array.isArray(parsedData.connections)
        ) {
          loadWorkflow(parsedData);
        } else {
          console.warn("Invalid workflow data in localStorage.");
        }
      }
    } catch (error) {
      console.error("Failed to load workflow from localStorage:", error);
      localStorage.removeItem("workflowData");
    }
  }, [loadWorkflow]);

  // Corrected getDatabaseDriver
  const getDatabaseDriver = (provider?: string): string => {
    const drivers: Record<string, string> = {
      postgresql: "org.postgresql.Driver",
      mysql: "com.mysql.cj.jdbc.Driver",
      sqlserver: "com.microsoft.sqlserver.jdbc.SQLServerDriver",
      oracle: "oracle.jdbc.driver.OracleDriver",
      sqlite: "org.sqlite.JDBC",
      local: "org.postgresql.Driver", // Default 'local' to SQLite, adjust if different
    };
    return (
      drivers[provider?.toLowerCase() || "postgresql"] ||
      "org.postgresql.Driver"
    ); // Default to postgresql
  };

  // --- REVISED saveAndRunWorkflow ---
  const saveAndRunWorkflow = useCallback(async () => {
    const dynamicClientIdString = getCurrentClientId();
    if (!dynamicClientIdString) {
      toast({
        title: "Error",
        description: "No client ID. Select client first.",
        variant: "destructive",
      });
      return;
    }
    const clientId = Number.parseInt(dynamicClientIdString, 10);
    if (isNaN(clientId)) {
      toast({
        title: "Error",
        description: "Invalid client ID.",
        variant: "destructive",
      });
      return;
    }
    const currentWorkflowId = getCurrentWorkflowId();
    if (!currentWorkflowId) {
      toast({
        title: "Error",
        description: "No workflow ID. Create workflow first.",
        variant: "destructive",
      });
      return;
    }
    if (nodes.length === 0) {
      toast({
        title: "Error",
        description: "Empty workflow. Add nodes.",
        variant: "destructive",
      });
      return;
    }

    try {
      const startNodesList = nodes.filter((node) => node.type === "start");
      const endNodesList = nodes.filter((node) => node.type === "end");
      if (startNodesList.length === 0 || endNodesList.length === 0) {
        toast({
          title: "Error",
          description: "Workflow needs start and end nodes.",
          variant: "destructive",
        });
        return;
      }

      const readFileNodes = nodes.filter((node) => node.type === "read-file");
      const writeFileNodes = nodes.filter((node) => node.type === "write-file");
      const databaseNodes = nodes.filter((node) => node.type === "database");
      const databaseSourceNodes = nodes.filter(
        (node) => node.type === "source"
      );
      const copyFileNodes = nodes.filter((node) => node.type === "copy-file");
      const moveFileNodes = nodes.filter((node) => node.type === "move-file");
      const renameFileNodes = nodes.filter(
        (node) => node.type === "rename-file"
      );
      const deleteFileNodes = nodes.filter(
        (node) => node.type === "delete-file"
      );
      const filterNodes = nodes.filter((node) => node.type === "filter");

      let dagSequence: any[] = [];
      let createdConfigId: number | null = null;
      let operationTypeForDag: "file_conversion" | "cli_operator" | null = null;
      let configPayload: any = null; // Generic payload for either type

      // --- FILE-TO-FILE ---
      if (readFileNodes.length > 0 && writeFileNodes.length > 0) {
        console.log("Detected: File-to-File");
        operationTypeForDag = "file_conversion";
        const readNode = readFileNodes[0];
        const writeNode = writeFileNodes[0];
        const filterNode = filterNodes.length > 0 ? filterNodes[0] : null;
        if (!readNode.data.path || !writeNode.data.path) {
          toast({
            title: "Error",
            description: "File-to-File: Input/Output paths required.",
            variant: "destructive",
          });
          return;
        }
        configPayload = {
          input: {
            provider: readNode.data.provider || "local",
            format: readNode.data.format || "csv",
            path: readNode.data.path,
            options: readNode.data.options || {},
            schema: readNode.data.schema,
          },
          output: {
            provider: writeNode.data.provider || "local",
            format: writeNode.data.format || "parquet",
            path: writeNode.data.path,
            mode: writeNode.data.writeMode || "overwrite",
            options: writeNode.data.options || {},
          },
          filter: filterNode
            ? {
                operator: filterNode.data.operator || "and",
                conditions: filterNode.data.conditions || [],
              }
            : undefined,
          dag_id: currentWorkflowId,
        };
      }
      // --- FILE-TO-DATABASE ---
      else if (readFileNodes.length > 0 && databaseNodes.length > 0) {
        console.log("Detected: File-to-Database");
        operationTypeForDag = "file_conversion";
        const readNode = readFileNodes[0];
        const databaseNode = databaseNodes[0];
        const filterNode = filterNodes.length > 0 ? filterNodes[0] : null;
        if (
          !readNode.data.path ||
          !databaseNode.data.connectionString ||
          !databaseNode.data.table
        ) {
          toast({
            title: "Error",
            description:
              "File-to-DB: File path, DB connection, and table required.",
            variant: "destructive",
          });
          return;
        }
        configPayload = {
          input: {
            provider: readNode.data.provider || "local",
            format: readNode.data.format || "csv",
            path: readNode.data.path,
            options: readNode.data.options || {},
            schema: readNode.data.schema,
          },
          output: {
            provider:
              databaseNode.data.provider === "local"
                ? "sqlite"
                : databaseNode.data.provider,
            format: "sql",
            path: databaseNode.data.connectionString,
            mode: databaseNode.data.writeMode || "overwrite",
            options: {
              table: databaseNode.data.table,
              user: databaseNode.data.user || "",
              password: databaseNode.data.password || "",
              batchSize: databaseNode.data.batchSize || "5000",
              driver: getDatabaseDriver(databaseNode.data.provider),
            },
          },
          filter: filterNode
            ? {
                operator: filterNode.data.operator || "and",
                conditions: filterNode.data.conditions || [],
              }
            : undefined,
          dag_id: currentWorkflowId,
        };
      }
      // --- DATABASESOURCE-TO-FILE ---
      else if (databaseSourceNodes.length > 0 && writeFileNodes.length > 0) {
        // << ENSURE THIS LINE IS CORRECT
        console.log("Detected: DatabaseSource('source' node)-to-File"); // << THIS LOG SHOULD APPEAR
        operationTypeForDag = "file_conversion";
        const dbSourceNode = databaseSourceNodes[0];
        const writeNode = writeFileNodes[0];
        const filterNode = filterNodes.length > 0 ? filterNodes[0] : null;

        // Validation using the correct field names from your node data
        if (
          !dbSourceNode.data.connectionString ||
          (!dbSourceNode.data.query && !dbSourceNode.data.table) || // Check 'tableName'
          !writeNode.data.path
        ) {
          toast({
            title: "Error",
            description:
              "DB-to-File: DB conn, (query or table name), and output path required.",
            variant: "destructive",
          });
          return; // Exit if validation fails
        }

        // Determine the actual database provider
        // const actualDbProvider =
        //   // dbSourceNode.data.databaseprovider ||
        //   dbSourceNode.data.provider || "postgresql";

        configPayload = {
          input: {
            provider: dbSourceNode.data.provider || "postgresql",
            format: "sql",
            path: dbSourceNode.data.connectionString,
            options: {
              // query:
              //   dbSourceNode.data.query ||
              //   (dbSourceNode.data.table
              //     ? `SELECT * FROM "${dbSourceNode.data.table}"`
              //     : undefined), // Use 'tableName'
              table: dbSourceNode.data.table,
              user: dbSourceNode.data.user || dbSourceNode.data.user || "", // Use 'username'
              password: dbSourceNode.data.password || "",

              driver: getDatabaseDriver(dbSourceNode.data.provider),
            },
            schema: dbSourceNode.data.schema,
          },
          output: {
            provider: writeNode.data.provider || "local",
            format: writeNode.data.format || "xml", // Assuming XML from your earlier payload example
            path: writeNode.data.path,
            mode: writeNode.data.writeMode || "overwrite",
            options: writeNode.data.options || {
              rootTag: "TableData",
              rowTag: "Row",
            }, // Default XML options
          },
          filter: filterNode
            ? {
                operator: filterNode.data.operator || "and",
                conditions: filterNode.data.conditions || [],
              }
            : undefined,
          dag_id: currentWorkflowId,
        };
      }
      // --- COPY FILE ---
      else if (copyFileNodes.length > 0) {
        console.log("Detected: Copy-File");
        operationTypeForDag = "cli_operator";
        const node = copyFileNodes[0];
        if (!node.data.source_path || !node.data.destination_path) {
          toast({
            title: "Error",
            description: "Copy: Source/Destination paths required.",
            variant: "destructive",
          });
          return;
        }
        configPayload = mapCopyFileToCliOperator(node);
      }
      // --- MOVE FILE ---
      else if (moveFileNodes.length > 0) {
        console.log("Detected: Move-File");
        operationTypeForDag = "cli_operator";
        const node = moveFileNodes[0];
        if (!node.data.source_path || !node.data.destination_path) {
          toast({
            title: "Error",
            description: "Move: Source/Destination paths required.",
            variant: "destructive",
          });
          return;
        }
        configPayload = mapMoveFileToCliOperator(node);
      }
      // --- RENAME FILE ---
      else if (renameFileNodes.length > 0) {
        console.log("Detected: Rename-File");
        operationTypeForDag = "cli_operator";
        const node = renameFileNodes[0];
        // Assuming rename also uses source_path for old and destination_path for new for consistency with mappers
        if (!node.data.source_path || !node.data.destination_path) {
          toast({
            title: "Error",
            description: "Rename: Old/New paths required.",
            variant: "destructive",
          });
          return;
        }
        configPayload = mapRenameFileToCliOperator(node);
      }
      // --- DELETE FILE ---
      else if (deleteFileNodes.length > 0) {
        console.log("Detected: Delete-File");
        operationTypeForDag = "cli_operator";
        const node = deleteFileNodes[0];
        if (!node.data.source_path) {
          toast({
            title: "Error",
            description: "Delete: Source path required.",
            variant: "destructive",
          });
          return;
        }
        configPayload = mapDeleteFileToCliOperator(node);
      }
      // --- NO MATCH ---
      else {
        console.log("No recognized workflow operation type found.");
        toast({
          title: "Error",
          description: "Unsupported workflow operation.",
          variant: "destructive",
        });
        return;
      }

      // Create configuration
      if (operationTypeForDag === "file_conversion") {
        console.log(
          "Creating file_conversion config with:",
          JSON.stringify(configPayload, null, 2)
        );
        const response = await createFileConversionConfig(
          clientId,
          configPayload
        );
        if (!response?.id)
          throw new Error(
            "Failed to create file conversion config or ID missing."
          );
        createdConfigId = response.id;
      } else if (operationTypeForDag === "cli_operator") {
        console.log(
          `Creating CLI operator config (${configPayload.operation}) with:`,
          JSON.stringify(configPayload, null, 2)
        );
        const response = await createCliOperatorConfig(clientId, configPayload);
        if (!response?.id)
          throw new Error(
            `Failed to create CLI op config for ${configPayload.operation} or ID missing.`
          );
        createdConfigId = response.id;
      }

      if (createdConfigId === null || !operationTypeForDag) {
        toast({
          title: "Error",
          description: "Config creation failed.",
          variant: "destructive",
        });
        return;
      }

      const taskNodeIdPrefix =
        operationTypeForDag === "file_conversion" ? "fc_node_" : "cli_op_node_";
      dagSequence = [
        {
          id: makePythonSafeId(startNodesList[0].id),
          type: "start",
          config_id: 1,
          next: [`${taskNodeIdPrefix}${createdConfigId}`],
        },
        {
          id: `${taskNodeIdPrefix}${createdConfigId}`,
          type: operationTypeForDag,
          config_id: createdConfigId,
          next: [makePythonSafeId(endNodesList[0].id)],
        },
        {
          id: makePythonSafeId(endNodesList[0].id),
          type: "end",
          config_id: 1,
          next: [],
        },
      ];

      console.log(
        "Updating DAG with sequence:",
        JSON.stringify(dagSequence, null, 2)
      );
      const dagUpdateData = { dag_sequence: dagSequence, active: true };
      const updatedDag = await updateDag(currentWorkflowId, dagUpdateData);
      if (!updatedDag) throw new Error("Failed to update DAG on backend.");

      try {
        console.log("Triggering DAG run for workflow:", currentWorkflowId);
        const triggerResult = await triggerDagRun(currentWorkflowId);
        if (!triggerResult)
          console.warn(
            "DAG run trigger returned non-truthy value, but workflow saved."
          );
      } catch (triggerError) {
        console.error(
          "Error triggering DAG run (workflow saved):",
          triggerError
        );
        toast({
          title: "Partial Success",
          description: "Workflow saved; run trigger failed. Run manually.",
          variant: "default",
        });
      }

      toast({
        title: "Success",
        description: "Workflow saved and run triggered.",
      });
    } catch (error) {
      console.error("Error in saveAndRunWorkflow:", error);
      toast({
        title: "Workflow Error",
        description:
          error instanceof Error
            ? error.message
            : "Failed to save/run workflow.",
        variant: "destructive",
      });
    }
  }, [nodes, connections, getCurrentWorkflowId, toast, getDatabaseDriver]); // Added getDatabaseDriver

  const value: WorkflowContextType = {
    nodes,
    connections,
    logs,
    selectedNodeId,
    pendingConnection,
    propertiesModalNodeId,
    dataMappingModalNodeId,
    draggingNodeInfo,
    setPendingConnection,
    setPropertiesModalNodeId,
    setDataMappingModalNodeId,
    setDraggingNodeInfo,
    addNode,
    updateNode,
    removeNode,
    selectNode,
    addConnection,
    removeConnection,
    clearWorkflow,
    saveWorkflow,
    saveWorkflowToBackend,
    loadWorkflow,
    runWorkflow,
    executeNode,
    addLog,
    clearLogs,
    getNodeById,
    getCurrentWorkflowId,
    saveAndRunWorkflow,
  };

  return (
    <WorkflowContext.Provider value={value}>
      {children}
    </WorkflowContext.Provider>
  );
}

export function useWorkflow() {
  const context = useContext(WorkflowContext);
  if (context === undefined) {
    throw new Error("useWorkflow must be used within a WorkflowProvider");
  }
  return context;
}
